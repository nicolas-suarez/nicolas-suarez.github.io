<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nicolás Suárez Chavarría</title>
    <link>https://nicolas-suarez.github.io/</link>
      <atom:link href="https://nicolas-suarez.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Nicolás Suárez Chavarría</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 14 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://nicolas-suarez.github.io/media/icon_hu5c88e8a9398cf657f3e83a0d9ccab3ac_75464_512x512_fill_lanczos_center_3.png</url>
      <title>Nicolás Suárez Chavarría</title>
      <link>https://nicolas-suarez.github.io/</link>
    </image>
    
    <item>
      <title>Predicting asset ownership in Africa using satellite imagery</title>
      <link>https://nicolas-suarez.github.io/research/asset-ownership/</link>
      <pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/asset-ownership/</guid>
      <description>&lt;p&gt;This was our class project for Stanford CS229 &amp;ldquo;Machine Learning&amp;rdquo; class during the Fall 2022 quarter. You can find our poster submission &lt;a href=&#34;https://nicolas-suarez.github.io/files/asset_poster.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We predict asset ownership in Africa making use of Landsat-8 satellite imagery and ground truth data from the Demographic and Health Surveys (DHS). We trained a ResNet-18 model} (He et al., 2016), a sub-class of a Convolutional Neural Network (CNN), and used it to predict asset ownership levels over 6.72 km by 6.72 km patches of land in Africa. We used transfer learning to train our model to perform a regression task, initializing our model with weights originally used to classify images. The performance of our best model is much better than the performance of our baseline models, but our model might be overfitting, and the predictions it generates are far off from our ground truth data. We obtained lower $R^2$ validation set values than Yeh et al. (2020), a paper that attempts our same task.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;h2 id=&#34;ground-truth-demographic-and-health-surveys-dhs-surveys&#34;&gt;Ground truth: Demographic and Health Surveys (DHS) surveys&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We combined all the household DHS surveys for African countries that have been published since 2014 that have a matching file with the geocoordinates of each cluster.&lt;/li&gt;
&lt;li&gt;Our variable of interest is the score in the wealth index obtained by each household. This score is produced using Principal Component Analysis over  survey questions, including but not limited to access to drinkable water, sewerage, electricity, and ownership of farming and non-farming assets.&lt;/li&gt;
&lt;li&gt;We geocoded 12,511 locations across 24 countries, combining data from 26 DHS surveys between 2014 and 2021.&lt;/li&gt;
&lt;li&gt;In Panel (a) of Figure 1 we show the spatial distribution of our ground truth data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;satellite-images-landsat-8-surface-reflectance-collection&#34;&gt;Satellite images: Landsat-8 Surface Reflectance collection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We retrieved imagery using the Google Earth Engine API. For each DHS cluster with a geocoded location, we defined a patch of 6.72 km by 6.72 km centered in our location, and we retrieved an image for the patch using the Landsat 8 Surface Reflectance Tier 1 Collection.&lt;/li&gt;
&lt;li&gt;We used 3 bands from this collection: Red, Green and Blue surface reflectance. We preprocessed each of our images by adding a cloud mask per pixel and then computing the per pixel and band mean composite of all the available images for the year when the DHS cluster was surveyed.&lt;/li&gt;
&lt;li&gt;We retrieved images for 12,426 locations across all Africa. In Figure Panels (b) and (c) of Figure 1 we show some examples of the cloud masked imagery we produced.&lt;/li&gt;
&lt;li&gt;We used an 80%, 10%, 10% training, validation and test data split. In the following table we show the sample sizes of our 3 sets:&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Set&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Training&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Validation&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Observations&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9,941&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1,243&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1,242&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/asset_1_hu29b0469db94ec1933c1f284176f93c84_165256_4ecb1c08eeb24d6e11349aaa80e7597b.jpg 400w,
               /media/asset_1_hu29b0469db94ec1933c1f284176f93c84_165256_d46adf3cd3a7b235ed9c9d4bf4a7f7ad.jpg 760w,
               /media/asset_1_hu29b0469db94ec1933c1f284176f93c84_165256_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/asset_1_hu29b0469db94ec1933c1f284176f93c84_165256_4ecb1c08eeb24d6e11349aaa80e7597b.jpg&#34;
               width=&#34;760&#34;
               height=&#34;638&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We trained a Residual Network, a particular kind of a Convolutional Neural Network (CNN). We use transfer learning to initialize our model, and we adapt its architecture so we can use it for a regression task.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Residual Network (ResNet)}&lt;/strong&gt;: Type of CNN with a special architecture defined by  He et al. (2016). These kind of models are build by stacking &lt;strong&gt;Residual Blocks&lt;/strong&gt;, blocks of convolutional layers connected with activation functions, where the input of the block is then added to the output of the stacked convolutional layers at the end (see example in Figure 2). Residual blocks should help a deep CNN to avoid the performance problems associated with very deep networks, so if some of the final layers are not helping the model performance, their weights will be set to 0 and the block will become an identity mapping.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/asset_2_hu36c954061ed853e24213281a960c0d16_36256_a773d17cc55538e60744a26494f47646.jpg 400w,
               /media/asset_2_hu36c954061ed853e24213281a960c0d16_36256_3a2048aca87d1a2518d6449be91f42a0.jpg 760w,
               /media/asset_2_hu36c954061ed853e24213281a960c0d16_36256_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/asset_2_hu36c954061ed853e24213281a960c0d16_36256_a773d17cc55538e60744a26494f47646.jpg&#34;
               width=&#34;698&#34;
               height=&#34;382&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Our implementation&lt;/strong&gt;: We trained a ResNet-18 model, a Residual Network that contains 18 convolutional layers grouped in 8 residual blocks connected by ReLU activation functions. The last layer of the original model is a linear layer connected to a softmax layer that classifies images into 1,000 classes, so we modify the last linear layer so now it outputs only one number that will represent our predicted asset ownership, and we measure our loss as our root mean squared error (RMSE). We initialize our model using the original ResNet-18 pretrained weights. In Figure 3 we show our model&amp;rsquo;s architecture:&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/asset_3_hu8dee5766a402a1e0bac9cd4686f0c167_109232_abc449433de29b70a77d0c9af0bf8f36.jpg 400w,
               /media/asset_3_hu8dee5766a402a1e0bac9cd4686f0c167_109232_2aaf7136309fcee34bced077019c3059.jpg 760w,
               /media/asset_3_hu8dee5766a402a1e0bac9cd4686f0c167_109232_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/asset_3_hu8dee5766a402a1e0bac9cd4686f0c167_109232_abc449433de29b70a77d0c9af0bf8f36.jpg&#34;
               width=&#34;760&#34;
               height=&#34;327&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Benchmark models&lt;/strong&gt;: We trained linear regression, Lasso regression and Ridge regression models to compare the performance of our model against them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;To tune the hyperparameters of our models, we experimented using data augmentation techniques and modifying several components of our model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data augmentation techniques&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Applying independent vertical and horizontal flips to our images with certain probabilities.&lt;/li&gt;
&lt;li&gt;Rotating our images in degrees multiples of 90 (0, 90, 180 and 270) with certain probabilities.&lt;/li&gt;
&lt;li&gt;Applying different degrees of Gaussian Blurring to our images.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hyperparameters tuned:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Number of epochs for training.&lt;/li&gt;
&lt;li&gt;Mini-batch training size.&lt;/li&gt;
&lt;li&gt;Number of frozen convolutional layers at the bottom of the model (last layers of the model).&lt;/li&gt;
&lt;li&gt;Optimizer (Stochastic Gradient Descent or Adam).&lt;/li&gt;
&lt;li&gt;Learning rate.&lt;/li&gt;
&lt;li&gt;For Stochastic Gradient Descent, we tuned the momentum parameter (momentum makes our gradient a moving average of our previous gradients).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;We computed the RMSE and $R^2$ coefficient in our &lt;strong&gt;training&lt;/strong&gt;, &lt;strong&gt;validation&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; sets for our CNN and benchmark models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression and Ridge regression have good performance on the training set but a very poor performance on the validation and test sets.&lt;/li&gt;
&lt;li&gt;LASSO regression performs equally bad on the training, validation and test sets.
Our best model is a &lt;strong&gt;ResNet-18&lt;/strong&gt; model trained with:&lt;/li&gt;
&lt;li&gt;200 epochs&lt;/li&gt;
&lt;li&gt;Training batch size of 500 images&lt;/li&gt;
&lt;li&gt;4 data augmentation transformations applied to images&lt;/li&gt;
&lt;li&gt;Adam optimizer&lt;/li&gt;
&lt;li&gt;Learning rate = 0.001
The best model has an excellent performance on the training set, and a good performance on the validation and test sets, suggesting overfitting despite using regularization techniques.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/asset_5_huad5386ebdd1640fc1aae4011f8eb8382_74957_88e951855976fb9beb1ce6a454213d63.jpg 400w,
               /media/asset_5_huad5386ebdd1640fc1aae4011f8eb8382_74957_28dbbe53c213fc71dd2146614c889324.jpg 760w,
               /media/asset_5_huad5386ebdd1640fc1aae4011f8eb8382_74957_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/asset_5_huad5386ebdd1640fc1aae4011f8eb8382_74957_88e951855976fb9beb1ce6a454213d63.jpg&#34;
               width=&#34;662&#34;
               height=&#34;340&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h3 id=&#34;country-average-asset-ownership-for-pixels-in-our-test-set&#34;&gt;Country average asset ownership for pixels in our test set&lt;/h3&gt;
&lt;p&gt;Our results are off from the ground truth. Most of the ground truth averages are negative, but we predict on average positive values of asset ownership.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/asset_4_hu38952b2385dfb143bb0c6e98a7340310_104807_e096b15545effeefa20f2f48508594c6.jpg 400w,
               /media/asset_4_hu38952b2385dfb143bb0c6e98a7340310_104807_aba89d39a0116df39d6d9da06a8e5d69.jpg 760w,
               /media/asset_4_hu38952b2385dfb143bb0c6e98a7340310_104807_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/asset_4_hu38952b2385dfb143bb0c6e98a7340310_104807_e096b15545effeefa20f2f48508594c6.jpg&#34;
               width=&#34;760&#34;
               height=&#34;586&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;concluding-remarks-and-future-work&#34;&gt;Concluding remarks and future work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We trained a ResNet-18 model with Landsat-8 satellite imagery to predict asset ownership in Africa. We modified the original ResNet-18 architecture so we could train it for a regression task, and we used transfer learning to take advantage of the pretrained ResNet-18 weights.&lt;/li&gt;
&lt;li&gt;The performance of our best model is much better than the performance of our baseline models, but the model might be overfitting, and the predictions it generates are off from our ground truth data. We obtained lower $R^2$ validation set values than Yeh et al. (2020).&lt;/li&gt;
&lt;li&gt;Future work on this might include expanding our sample size by including more years with DHS surveys, using more channels of our satellite images, increasing the regularization in our model, adding dropout probabilities to our nodes and tuning that hyperparameter.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Generalized 2SLS implementation in Stata</title>
      <link>https://nicolas-suarez.github.io/blog/peer_iv/</link>
      <pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/blog/peer_iv/</guid>
      <description>&lt;p&gt;For one of my research projects I was working on peer effects for college students, and I wanted to use the Generalized 2SLS procedure to estimate peer effects described in &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0304407609000335&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bramoullé, Y., Djebbari, H., &amp;amp; Fortin, B. (2009). Identification of peer effects through social networks. Journal of econometrics, 150(1), 41-55.&lt;/a&gt;. The problem is that this project required a lot of data work and cleaning, so I worked on it on Stata mostly, but there is not a lot of support for peer effects regressions on &lt;code&gt;Stata&lt;/code&gt;, so I had to adapt the code of Bramoullé et al. (2009) on my own. I based my code on their article, and on the code shared on 
&lt;a href=&#34;https://rpubs.com/Nicolas_Gallo/549370&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Habiba Djebbari&amp;rsquo;s website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here I share my code, some examples about how to use it, and a comparison of my code and the original &lt;code&gt;R&lt;/code&gt; code used by the authors.&lt;/p&gt;
&lt;p&gt;To wrote this, I took advantage of Stata&amp;rsquo;s new integration with Python and Jupyter Notebooks (more information on how to use this feature can be found &lt;a href=&#34;https://www.stata.com/new-in-stata/jupyter-notebooks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;). I also used the &lt;a href=&#34;https://rpy2.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rpy2 package&lt;/a&gt; to run &lt;code&gt;R&lt;/code&gt; commands inside a Jupyter notebook.&lt;/p&gt;
&lt;p&gt;The original Jupyter Notebook I wrote for this can be found in my &lt;a href=&#34;https://github.com/nicolas-suarez/peer_iv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-stata-in-python&#34;&gt;Setting up Stata in Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#setting up Stata in Python
import stata_setup
stata_setup.config(&amp;quot;C:/Program Files/Stata17&amp;quot;, &amp;quot;mp&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  ___  ____  ____  ____  ____ ®
 /__    /   ____/   /   ____/      17.0
___/   /   /___/   /   /___/       MP—Parallel Edition

 Statistics and Data Science       Copyright 1985-2021 StataCorp LLC
                                   StataCorp
                                   4905 Lakeway Drive
                                   College Station, Texas 77845 USA
                                   800-STATA-PC        https://www.stata.com
                                   979-696-4600        stata@stata.com

Stata license: Unlimited-user 4-core network, expiring 21 Jul 2022
Serial number: xxxxxxxxxxxx
  Licensed to: Nicolas Suarez
               Stanford University

Notes:
      1. Unicode is supported; see help unicode_advice.
      2. More than 2 billion observations are allowed; see help obs_advice.
      3. Maximum number of variables is set to 5,000; see help set_maxvar.

Running C:\Program Files\Stata17/profile.do ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;peer-iv-function&#34;&gt;Peer IV function&lt;/h1&gt;
&lt;p&gt;To implement linear in means regressions, I wrote the &lt;code&gt;peer_iv&lt;/code&gt; function. This function takes as inputs, as any other regression function, a dependent variable and independent variables, plus an adjacency matrix &lt;code&gt;G&lt;/code&gt; to perform the calculations. This matrix has to be a &lt;code&gt;Mata&lt;/code&gt; matrix object (&lt;code&gt;Stata&lt;/code&gt; matrices have size limitations), and it is the only parameter that the user is forced to set. All the other parameters are optiontal: &lt;code&gt;row&lt;/code&gt; allows us to row-normalize the adjacency matrix (so the sum of each row is 1, and we can interpret the product of a variable and the matrix as weighted means), the &lt;code&gt;fixed&lt;/code&gt; option adds group level fixed effects, the &lt;code&gt;ols&lt;/code&gt; option runs a regular OLS regression but with peer effects, so by using this we avoid creating new variables with average outcomes and regressors for the peers of each observation, and the &lt;code&gt;vardir&lt;/code&gt; option allow us to pass variables directly to the regression with the &lt;code&gt;ols&lt;/code&gt; option, so they won&amp;rsquo;t have a peer effect (this can be used for instance to add dummies for fixed effects).&lt;/p&gt;
&lt;p&gt;The matrix generates an standard Stata regression output, containing coefficients, standard errors, p-values and all the relevant information, and it stores eclass results. This means that the output could be stored with custom commands like &lt;code&gt;outreg2&lt;/code&gt;, &lt;code&gt;estout&lt;/code&gt; or &lt;code&gt;esttab&lt;/code&gt; that allow the user to build customizable output tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%stata

capture program drop peer_iv
program define peer_iv, eclass
    version 17
    syntax varlist, ADJacency(name) [ROW FIXED OLS VARdir(varlist)] 
    /* implements the generalized 2SLS model of Bramoulle et al (2009), without fixed effects.
    The model includes a constant, then the endogenous effect, effects of the independent variables, and then the exogenous effects.
    For more details see https://rpubs.com/Nicolas_Gallo/549370

    INPUTS:
    varlist = exogenous variables
    dep= dependent variable
    adjacency=name of the adjacency matrix in Mata
    row=optional, row normalizes the adjacency matrix
    fixed=optional, estimates model with cohort level fixed effects
    ols=optional, OLS results. Don&#39;t use together with FIXED
    vardir=optional. Variables to use in the regression that won&#39;t have peer effects, only to be used with OLS.

    OUTPUT:
    displays the coefficients and standard errors in a table. Stores eclass results.
    */
    *separating dependent from independent variables
    gettoken dep varlist: varlist
    preserve
    quietly{

    *returning error if OLS and FIXED are used together
    if &amp;quot;`ols&#39;&amp;quot;!=&amp;quot;&amp;quot; &amp;amp; &amp;quot;`fixed&#39;&amp;quot;!=&amp;quot;&amp;quot; { 
    noisily display as error &amp;quot;options OLS and FIXED may not be combined&amp;quot;
    exit 184
    }

    *returning error if VARDIR and OLS are not used together
    if &amp;quot;`ols&#39;&amp;quot;==&amp;quot;&amp;quot;  &amp;amp; &amp;quot;`vardir&#39;&amp;quot;!=&amp;quot;&amp;quot; { 
    noisily display as error &amp;quot;option VARDIR has to be used with OLS option&amp;quot;
    exit 184
    }

    *checking if there are missing values in our data
    reg `dep&#39; `varlist&#39; `vardir&#39;
    *recovering the indexes of non-missing observations
    gen muestra=e(sample)
    ereturn clear

    *moving data as matrices
    mata X=st_data(.,&amp;quot;`varlist&#39;&amp;quot;)
    mata y=st_data(.,&amp;quot;`dep&#39;&amp;quot;)
    mata muestra=st_data(.,&amp;quot;muestra&amp;quot;)
    if &amp;quot;`vardir&#39;&amp;quot;!=&amp;quot;&amp;quot; mata vardir=st_data(.,&amp;quot;`vardir&#39;&amp;quot;)
    
    *dropping missing values from data matrices
    mata X=select(X,muestra)
    mata y=select(y,muestra)
    if &amp;quot;`vardir&#39;&amp;quot;!=&amp;quot;&amp;quot; mata vardir=select(vardir,muestra)

    *dropping missing values from G matrix (eliminating the rows and columns with missing values, so the matrix are comformable)
    mata G1=select(`adjacency&#39;,muestra)
    mata G1=select(G1,muestra&#39;)

    *row normalizing G if needed
    if &amp;quot;`row&#39;&amp;quot;!=&amp;quot;&amp;quot; mata G1=G1:/editvalue(rowsum(G1),0,1)

    *generating identity matrix
    mata Id=I(rows(G1))

    *OLS results
    if &amp;quot;`ols&#39;&amp;quot;!=&amp;quot;&amp;quot; {
    if &amp;quot;`vardir&#39;&amp;quot;!=&amp;quot;&amp;quot; mata X_1 =  J(rows(X),1,1), G1*y, X, G1*X, vardir
    else mata X_1 =  J(rows(X),1,1), G1*y, X, G1*X
    mata theta= invsym(quadcross(X_1, X_1))*quadcross(X_1, y)
    mata e= y - X_1*theta
    mata V = (quadsum(e:^2)/(rows(X_1)-cols(X_1)))*invsym(quadcross(X_1, X_1))
    }
    else {
    *putting matrices together
    *with fixed effects
    if &amp;quot;`fixed&#39;&amp;quot;!=&amp;quot;&amp;quot; {
    mata S=( (Id-G1)*X, (Id-G1)*G1*X, (Id-G1)*G1*G1*X )
    mata X_1= ( (Id-G1)*G1*y, (Id-G1)*X, (Id-G1)*G1*X )             
    }
    else{
    mata S=( J(rows(X),1,1), X, G1*X, G1*G1*X )
    mata X_1= ( J(rows(X),1,1), G1*y, X, G1*X )
    }
    mata P= S*invsym(quadcross(S,S))*S&#39;

    *first 2sls
    if &amp;quot;`fixed&#39;&amp;quot;!=&amp;quot;&amp;quot; mata theta_1= invsym(X_1&#39;*P*X_1)*X_1&#39;*P*(Id-G1)*y
    else mata theta_1= invsym(X_1&#39;*P*X_1)*X_1&#39;*P*y

    *building instrument
    if &amp;quot;`fixed&#39;&amp;quot;!=&amp;quot;&amp;quot; {
    mata Z = G1*luinv(Id-theta_1[1]*G1)*(Id-G1)*(X*theta_1[2::(1+cols(X))] +  G1*X*theta_1[(2+cols(X))::(1+2*cols(X))] ), (Id-G1)*X, (Id-G1)*G1*X   
    }
    else{
    mata Z = J(rows(X),1,1), G1*luinv(Id-theta_1[2]*G1)*( theta_1[1]*J(rows(X),1,1) + X*theta_1[3::(2+cols(X))] +  G1*X*theta_1[(3+cols(X))::(2+2*cols(X))] ), X, G1*X
    }
    *

    *final 2sls
    if &amp;quot;`fixed&#39;&amp;quot;!=&amp;quot;&amp;quot; mata theta = luinv(quadcross(Z,X_1))*quadcross(Z,(Id-G1)*y)
    else mata theta = luinv(quadcross(Z,X_1))*quadcross(Z,y)

    *resids
    if &amp;quot;`fixed&#39;&amp;quot;!=&amp;quot;&amp;quot; {
    mata e= (Id-G1)*y - luinv(Id-theta[1]*G1)*((Id-G1)*X*theta[2::(1+cols(X))] + (Id-G1)*G1*X*theta[(2+cols(X))::(1+2*cols(X))] )
    }
    else{
    mata e= y - luinv(Id-theta[2]*G1)*( theta[1]*J(rows(X),1,1) + X*theta[3::(2+cols(X))] +  G1*X*theta[(3+cols(X))::(2+2*cols(X))] )
    }


    *variance
    mata V = luinv(quadcross(Z,X_1))*(Z&#39;)*diag(e:^2)*Z*luinv(quadcross(X_1,Z))
    }

    *sending results to Stata
    mata st_matrix(&amp;quot;b&amp;quot;,theta&#39;)
    mata st_matrix(&amp;quot;V&amp;quot;,V)

    *row and col names for matrices
    local exog_peer //list for names of exogenous effects
    foreach var in `varlist&#39;{
    local exog_peer `exog_peer&#39; `var&#39;_p
    }
    if &amp;quot;`fixed&#39;&amp;quot;!=&amp;quot;&amp;quot; {
    local varnames `dep&#39;_p `varlist&#39; `exog_peer&#39; `vardir&#39;
    }
    else{
    local varnames _cons `dep&#39;_p `varlist&#39; `exog_peer&#39; `vardir&#39;
    }


    *adding col and rownames
    matrix colnames b= `varnames&#39;
    matrix colnames V = `varnames&#39;
    matrix rownames V = `varnames&#39;
    }
    *storing eclass results
    *tab muestra
    ereturn post b V, depname(`dep&#39;) esample(muestra)
    mata st_numscalar(&amp;quot;e(N)&amp;quot;, rows(G1))
    mata st_numscalar(&amp;quot;e(df_r)&amp;quot;, rows(X_1)-cols(X_1))
    eret local cmd peer_iv
    ereturn display

    restore     
end

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Here we will run a little example to see how the command works, and how you can generate an adjacency matrix in Stata. We are going to use the &lt;code&gt;auto&lt;/code&gt; dataset with 1978 automobile data, and we are going to create a random adjacency matrix &lt;code&gt;G&lt;/code&gt; with elements that are drawn from a uniform between 0 an 1, but we will force the elements in the main diagonal to be 0. We are also going to row normalize the adjacency matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%stata

sysuse auto, clear

*generating the matrix
mata G= runiform(`c(N)&#39;, `c(N)&#39;) 
forval i=1/`c(N)&#39;{
    mata G[strtoreal(st_local(&amp;quot;i&amp;quot;)),strtoreal(st_local(&amp;quot;i&amp;quot;))]=0
}

*running the regression
peer_iv price trunk turn, row adj(G)

eret list
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;. 
. sysuse auto, clear
(1978 automobile data)

. 
. *generating the matrix
. mata G= runiform(`c(N)&#39;, `c(N)&#39;) 

. forval i=1/`c(N)&#39;{
  2.         mata G[strtoreal(st_local(&amp;quot;i&amp;quot;)),strtoreal(st_local(&amp;quot;i&amp;quot;))]=0
  3. }

. 
. *running the regression
. peer_iv price trunk turn, row adj(G)
------------------------------------------------------------------------------
       price | Coefficient  Std. err.      t    P&amp;gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
       _cons |   41706.23   48765.18     0.86   0.395    -55603.18    139015.6
     price_p |  -1.527064   11.64602    -0.13   0.896    -24.76633     21.7122
       trunk |   141.5504   83.60055     1.69   0.095    -25.27191    308.3727
        turn |    98.8758   103.6205     0.95   0.343    -107.8956    305.6472
     trunk_p |     439.69   968.8704     0.45   0.651    -1493.661    2373.041
      turn_p |  -959.4334   2872.551    -0.33   0.739    -6691.519    4772.652
------------------------------------------------------------------------------

. 
. eret list

scalars:
                  e(N) =  74
               e(df_r) =  68

macros:
                e(cmd) : &amp;quot;peer_iv&amp;quot;
         e(properties) : &amp;quot;b V&amp;quot;
             e(depvar) : &amp;quot;price&amp;quot;

matrices:
                  e(b) :  1 x 6
                  e(V) :  6 x 6

. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the output of the regression, including a constant, the coefficient &lt;code&gt;price_p&lt;/code&gt; is the coefficient of the endogenous effect, while &lt;code&gt;trunk_p&lt;/code&gt; and &lt;code&gt;turn_p&lt;/code&gt; are the exogenous effects.&lt;/p&gt;
&lt;p&gt;After the regression command we ran the &lt;code&gt;eret list&lt;/code&gt; command, and we can see all the elements that are stored after running the command.&lt;/p&gt;
&lt;h2 id=&#34;storing-mata-matrices&#34;&gt;Storing Mata matrices&lt;/h2&gt;
&lt;p&gt;At least for my particular application, computing the adjacency matrix was very slow, so it is not something that I would do each time before I want to run peer effects regressions. To avoid this, we can use the &lt;code&gt;Mata&lt;/code&gt; functions &lt;code&gt;matsave&lt;/code&gt; and &lt;code&gt;matuse&lt;/code&gt; to store a matrix as a &lt;code&gt;.mmat&lt;/code&gt; object, and then load it into Stata.&lt;/p&gt;
&lt;h1 id=&#34;checking-if-the-code-works&#34;&gt;Checking if the code works&lt;/h1&gt;
&lt;p&gt;Here, to see if my code works properly, I will run the &lt;code&gt;R&lt;/code&gt; code provided by Bramoullé, Djebbari and Fortin (it can be found &lt;a href=&#34;https://rpubs.com/Nicolas_Gallo/549370&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) to generate data, then estimate peer effects with and without fixed effects, export their data to Stata, and then see if my function obtains the same coefficients.&lt;/p&gt;
&lt;p&gt;The code provided by the authors is meant to be used to run Monte Carlo simulations, so I made some modifications to keep only the relevant parts. Also, for both cases, the authors defined different data generating processes, so we will have 2 vectors of dependent variables, but all of them are generated with the vector of white noise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Python package to use magic R commands
%load_ext rpy2.ipython
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;generating-network-data-in-r&#34;&gt;Generating network data in R&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;%%R

library(knitr)
library(igraph)
library(truncnorm)
set.seed(1)

alpha=0.7683
beta=0.4666
gamma=0.0834
delta=0.1507
e_var=0.1
#Generating a graph with 100 vertices and a probability of link of 0.04 with the &amp;quot;random.renyi.game()&amp;quot; function
g&amp;lt;-erdos.renyi.game(100,0.04)
#Generating the associated weighted adjacency matrix
G&amp;lt;-get.adjacency(g)
G&amp;lt;-as.matrix(G)

#Drawing a vector x of characteristics
x_sim&amp;lt;-matrix(rbinom(n = nrow(G),size = 1,prob = 0.9458 ),nrow(G),1)
for(i in 1:nrow(x_sim)){
  if(x_sim[i,] != 0){
    x_sim[i,]&amp;lt;-rtruncnorm(n = 1,a = 0,b = 1000,mean = 1,sd = 3) 
  }
}
#a vector filled with 1,  size m x 1 (used when there is an intercept in the model, i.e. when fixed_effects = FALSE)
l&amp;lt;-matrix(1,nrow(G),1)

GX&amp;lt;-G %*% x_sim
G2X&amp;lt;-(G %*% G) %*% x_sim
#an identity matrix of appropriate size
I&amp;lt;-(diag(nrow(G))) 
# Inv corresponds to (I - Beta*G))^(-1)   in the reduced form(check equation (5))
# Solve function gives the inverse of a matrix
Inv&amp;lt;-solve(I - beta * G)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;case-without-fixed-effects&#34;&gt;Case without fixed effects&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;%%R

#the instrument vector of size m x 4
S&amp;lt;-matrix(c(l,x_sim,GX,G2X),nrow(x_sim),)

#P is the weighting matrix of size m x n
P&amp;lt;-S %*% solve(t(S) %*% S) %*% t(S)


eps&amp;lt;-matrix(rnorm(n = nrow(G),mean = 0,sd = e_var),nrow(G),1)
y&amp;lt;-alpha * Inv %*% l  + Inv %*% (gamma * I + delta * G) %*% x_sim  + Inv %*% eps
Gy&amp;lt;-G %*% y

#X tilde, size n x 4
X_t&amp;lt;-matrix(c(l,Gy,x_sim,GX),nrow(x_sim),)

#theta 2sls and extracting its parameters
th_2sls &amp;lt;-solve(t(X_t) %*% P %*% X_t) %*% t(X_t) %*% P %*% y
alpha_2sls&amp;lt;-th_2sls[1]
beta_2sls&amp;lt;-th_2sls[2]
gamma_2sls&amp;lt;-th_2sls[3]
delta_2sls&amp;lt;-th_2sls[4]

#Recalculate I with Beta_2sls
I_2sls&amp;lt;-solve((diag(nrow(G)) - beta_2sls * G ))  

#Gy estimated in theta 2sls
gy_2sls&amp;lt;- G %*% I_2sls %*% (alpha_2sls * l   + gamma_2sls * x_sim + GX * delta_2sls)

Z_th&amp;lt;-matrix(c(rep(1,nrow(G)),gy_2sls,x_sim,GX),nrow(x_sim),)

#THETAS
#theta lee
th_lee_1&amp;lt;-solve(t(Z_th) %*% X_t) %*% t(Z_th) %*% y
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;case-with-fixed-effects&#34;&gt;Case with fixed effects&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;%%R
IG &amp;lt;-(I - G)
#the instrument vector of size m x 3
S&amp;lt;-matrix(c(IG%*%x_sim,IG%*%GX,IG%*%G2X),nrow(G),)

#P is the weighting matrix of size m x m
P&amp;lt;-S %*% solve(t(S) %*% S) %*% t(S)

y2&amp;lt;- solve(IG) %*% Inv %*% (gamma*I + delta * G) %*% IG %*% x_sim + solve(IG) %*% Inv %*% IG %*% eps

#X tilde, size m x 3
X_t&amp;lt;-matrix(c(G%*%IG%*%y2 ,IG%*%x_sim,IG%*%GX),nrow(x_sim),)


#theta 2sls and extracting its parameters
th_2sls &amp;lt;-solve(t(X_t) %*% P %*% X_t) %*% t(X_t) %*% P %*% IG%*%y2
beta_2sls&amp;lt;-th_2sls[1]
gamma_2sls&amp;lt;-th_2sls[2]
delta_2sls&amp;lt;-th_2sls[3]

#Recalculate I with Beta_2sls
Inv_2sls&amp;lt;-solve(I - beta_2sls * G )
#IGy estimated in theta 2sls
IGy_2sls&amp;lt;-Inv_2sls %*% (gamma_2sls * I + delta_2sls * G) %*% IG %*% x_sim + Inv_2sls %*% IG %*% eps 

IG_Gy_2sls&amp;lt;- G %*% Inv_2sls %*% (IG %*% (x_sim * gamma_2sls + GX* delta_2sls))

Z_th&amp;lt;-matrix(c(IG_Gy_2sls,IG%*%x_sim,IG%*%GX),nrow(G),)


#THETAS
#theta lee
th_lee_2&amp;lt;-solve(t(Z_th) %*% X_t) %*% t(Z_th) %*% IG%*%y2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;storing-data-in-r-as-csv-and-reading-it-in-stata&#34;&gt;Storing data in R as CSV, and reading it in Stata&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;%%R
#storing data and adjacency matrix as CSV, to be readed in Stata later
write.csv(data.frame(x_sim,y,y2),&#39;data.csv&#39;,row.names = FALSE)
write.csv(G,&#39;adjacency.csv&#39;,row.names = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%stata
*reading adjacency matrix, and passing it to Mata
import delimited &amp;quot;adjacency.csv&amp;quot;, clear
putmata  G_r=(v*), replace

*reading data
import delimited &amp;quot;data.csv&amp;quot;, varnames(1) clear
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;. *reading adjacency matrix, and passing it to Mata
. import delimited &amp;quot;adjacency.csv&amp;quot;, clear
(encoding automatically selected: ISO-8859-2)
(100 vars, 100 obs)

. putmata  G_r=(v*), replace
(1 matrix posted)

. 
. *reading data
. import delimited &amp;quot;data.csv&amp;quot;, varnames(1) clear
(encoding automatically selected: ISO-8859-1)
(3 vars, 100 obs)

. 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;comparing-results-without-fixed-effects&#34;&gt;Comparing results without fixed effects&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%stata
peer_iv y x_sim, adj(G_r)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-----------------------------------------------------------------------------
&amp;gt; -
           y | Coefficient  Std. err.      t    P&amp;gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
       _cons |   .7693815   .0861937     8.93   0.000     .5982885    .9404746
         y_p |   .4668116   .0019521   239.13   0.000     .4629367    .4706865
       x_sim |   .0832526   .0174479     4.77   0.000     .0486188    .1178864
     x_sim_p |   .1501907   .0057371    26.18   0.000     .1388026    .1615789
------------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;%%R
th_lee_1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          [,1]
[1,] 0.7693815
[2,] 0.4668116
[3,] 0.0832526
[4,] 0.1501907
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;comparing-results-with-fixed-effects&#34;&gt;Comparing results with fixed effects&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%stata
peer_iv y2 x_sim, fixed adj(G_r)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;------------------------------------------------------------------------------
          y2 | Coefficient  Std. err.      t    P&amp;gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        y2_p |   .4663327   .0025075   185.97   0.000      .461356    .4713095
       x_sim |   .0841561   .0081916    10.27   0.000      .067898    .1004143
     x_sim_p |   .1500943   .0018714    80.20   0.000       .14638    .1538086
------------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;%%R
th_lee_2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           [,1]
[1,] 0.46633273
[2,] 0.08415615
[3,] 0.15009431
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, in both cases, the &lt;code&gt;peer_iv&lt;/code&gt; command produces the same estimates as the original package. Furthermore, both packages produce estimates that are very similar to the ones used to generate our data (for instance, we have that $\beta=0.4666$, and in both cases we get coefficients very close to that).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classroom composition and network effects: Evidence from a college special admission program in Chile</title>
      <link>https://nicolas-suarez.github.io/research/peer-effects/</link>
      <pubDate>Wed, 13 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/peer-effects/</guid>
      <description>&lt;p&gt;This is a preliminary draft of a research idea I used as my 2nd year paper as a PhD. in Economics student. &lt;a href=&#34;https://nicolas-suarez.github.io/blog/peer_iv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; you can find a blog entry I wrote with details of my &lt;code&gt;Stata&lt;/code&gt; implementation of the Generalized 2SLS procedure described by Bramoullé et al. (2009).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimized Regression Discontinuity Application: the effect of national institutions over local development</title>
      <link>https://nicolas-suarez.github.io/research/opt-rdd/</link>
      <pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/opt-rdd/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;This is my final project for &lt;strong&gt;Stanford&amp;rsquo;s ECON 293: Machine Learning and Causal Inference&lt;/strong&gt; for the Spring 2021 quarter. The code used for this project (in RMarkDown) can be found &lt;a href=&#34;https://github.com/nicolas-suarez/opt_rdd_africa/blob/main/markdown.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and a write up explaining the project more in detail is available &lt;a href=&#34;https://github.com/nicolas-suarez/opt_rdd_africa/blob/main/write-up.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this project revisit the findings of &lt;a href=&#34;https://academic.oup.com/qje/article-abstract/129/1/151/1897929?redirectedFrom=fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michalopoulos, S., &amp;amp; Papaioannou, E. (2014). National institutions and subnational development in Africa. The Quarterly journal of economics, 129(1), 151-213.&lt;/a&gt;. In the mentioned article, the authors explore the role of national institutions on local development levels for in Africa. They exploit the fact that the national boundaries of African countries drawn during their independence partitioned more than 200 ethnic groups across adjacent countries, so within the territory of an ethnic group we have individuals subjected to similar cultures, residing in homogeneous geographic areas, but that are exposed to different national institutions.&lt;/p&gt;
&lt;p&gt;We can use a Geographical regression discontinuity design (as the authors do in Section IV.C of their paper) to see the effect of exposure to different levels of institutions over local development levels, measured with satellite images of light density at night, and using as a running variable the distance to the national border partitioning a ethnicity.&lt;/p&gt;
&lt;p&gt;The authors use 3rd and 4th degree RD polynomials, define the bandwidth of their RD design in arbitrary ways, and there are some pixels were the distance to the border is not computed properly, so to overcome those flaws and check how robust their findings are, I plan to use the &lt;a href=&#34;https://github.com/swager/optrdd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;optrdd package&lt;/a&gt; from &lt;a href=&#34;https://arxiv.org/pdf/1705.01677.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens, G., &amp;amp; Wager, S. (2019). Optimized regression discontinuity designs. Review of Economics and Statistics, 101(2), 264-278.&lt;/a&gt; to estimate an Optimized Regression Discontinuity model, a data-driven model that doesn&amp;rsquo;t rely on polynomial or bandwidth definition.&lt;/p&gt;
&lt;h2 id=&#34;empirical-design&#34;&gt;Empirical design&lt;/h2&gt;
&lt;p&gt;Before diving into the issues with the dataset, I will first explain the empirical design for this project. Here, our unit of analysis are patches of 12.6 by 12.6 km of land in Africa. Our dependent variable is a dummy indicating if a pixel is lit or not, according to 2007-2008 satellite night lights measures. Our treatment variable is a dummy indicating if a pixel is on the side of the ethnic tribe territory with higher national institutions (measured as either Rule of Law or Control of Corruption).&lt;/p&gt;
&lt;p&gt;Besides that main variables, the authors also control for population density in their original article, and they also have information about the country and tribe where a pixel is located, and some information related to its geography, like elevation, presence of water, a malaria index, and similar stuff.&lt;/p&gt;
&lt;h2 id=&#34;data-sources&#34;&gt;Data sources&lt;/h2&gt;
&lt;p&gt;I obtained the original pixel-level dataset from &lt;a href=&#34;https://drive.google.com/file/d/1UZzwCmT7RZ7JCSx-NXAfu_-n5i6xkjRr/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stelios Michalopoulos&#39; website&lt;/a&gt;. I obtained the shapefiles for the ethnic tribes from &lt;a href=&#34;https://scholar.harvard.edu/files/nunn/files/murdock_shapefile.zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nathan Nunn&amp;rsquo;s website&lt;/a&gt;. The shapefile with the grid of pixels covering Africa was provided directly by Stelios Michalopoulos.&lt;/p&gt;
&lt;h2 id=&#34;problems-with-the-current-data&#34;&gt;Problems with the current data&lt;/h2&gt;
&lt;p&gt;In the current dataset downloaded from the authors website, there are some problems with how the distance to the border is calculated, and they are also missing a lot of pixels, since they discarded all uninhabited pixels. We can illustrate these problems by looking at the Bideyat tribe, located in the border shared among Egypt, Sudan, Chad and Libya. Most of the pixels are in Sudan and Chad, so the authors considered only those 2 countries for their analysis.&lt;/p&gt;
&lt;p&gt;In following figure we are going to plot the distance to the border for the pixels in Chad (left) and Sudan (right), where blue pixels are the closest, and yellow are the farthest from the national border. Here we can observe that the distance to the border is clearly calculated in the wrong way: we have around 10 pixels at the top of Sudan, in the frontier with Egypt, that are marked to be less than 50 km to the border between Sudan and Chad, but they are around 200 kilometers away from the mentioned border. Their distance to the border was very likely computed around the wrong border.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-3-1_hu27329ac2600b8d62f3b328d8c757929f_6781_5555af68bc8a795f3dde07d8f7d12ce8.png 400w,
               /research/opt-rdd/unnamed-chunk-3-1_hu27329ac2600b8d62f3b328d8c757929f_6781_df6b231c4638485f9b745bd86d663ccf.png 760w,
               /research/opt-rdd/unnamed-chunk-3-1_hu27329ac2600b8d62f3b328d8c757929f_6781_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-3-1_hu27329ac2600b8d62f3b328d8c757929f_6781_5555af68bc8a795f3dde07d8f7d12ce8.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;As mentioned before, another problem present in this dataset is that there are a lot of missing pixels: in our previous figure we can see a lot of areas with very little pixels. If we check how many pixels we have per country in the Bideyat territory: there are 598 pixels in Sudan, but only 38 pixels in Chad, and we even have 45 pixels in Egypt, a country that should not be considered here given the approach of the authors.&lt;/p&gt;
&lt;p&gt;The imbalance present here could be quite problematic, because we have relatively very little pixels in Chad compared to Sudan, and also because the pixels in Chad are not very close to the border, so a regression discontinuity analysis is not going to produce interesting results here.&lt;/p&gt;
&lt;h2 id=&#34;replicating-and-fixing-the-dataset&#34;&gt;Replicating and fixing the dataset&lt;/h2&gt;
&lt;p&gt;Given the previous problems, before trying to estimate causal effects, I&amp;rsquo;m going to rebuild from scratch the dataset, fixing some issues and expanding the number of units in our sample. I will do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I&amp;rsquo;m going to start by loading the original grid of pixels for Africa.&lt;/li&gt;
&lt;li&gt;To that grid I will add information about nightlights (average of the DMSP OLS for the years 2007 and 2008) and population density (Population density in 2000).&lt;/li&gt;
&lt;li&gt;After that, I will also used GIS methods to identify in which country and tribe is every pixel located.&lt;/li&gt;
&lt;li&gt;With that information, and to use a similar sample to the original one, for each partitioned tribe, I will compute the distance to the national border. To follow Michalopoulos &amp;amp; Papaioannou (2014), I will only consider the two biggest countries in term of area inside a tribe if there are more than 2 countries in the partitioned territory.&lt;/li&gt;
&lt;li&gt;Finally, using the country and tribe names, I will recover the Rule of Law and Control of Corruption treatment variables from the original dataset, as well as the clustering variable (variable used by the authors and defined in the original ethnographic Atlas of Murdock that groups similar tribes into bigger groups).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this procedure we pass from an original sample with 40,209 observations, to a new sample with valid information for 55,055 observations. To check that we did everything properly when rebuilding the dataset, in next figure we can see again the distance to the border for the Bideyat tribe, but for the new dataset. We can see that we have data for all pixels in both Chad and Sudan, and that the distance to the border is now computed with respect to the right border.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-8-1_hu5c5265bad635330ad9851353dc8b774d_7367_cdbd4dfe83e741cebfd7f02af746329e.png 400w,
               /research/opt-rdd/unnamed-chunk-8-1_hu5c5265bad635330ad9851353dc8b774d_7367_927e70bbc26c382950f2614c13debad2.png 760w,
               /research/opt-rdd/unnamed-chunk-8-1_hu5c5265bad635330ad9851353dc8b774d_7367_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-8-1_hu5c5265bad635330ad9851353dc8b774d_7367_cdbd4dfe83e741cebfd7f02af746329e.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;comparing-the-different-samples&#34;&gt;Comparing the different samples&lt;/h2&gt;
&lt;p&gt;I also replicate the original results of estimating equation (3) of Michalopoulos &amp;amp; Papaioannou (2014), that are displayed in their Table VI. To keep things short, those results are only available in the write-up and the knitted version of the RMarkDown code.&lt;/p&gt;
&lt;h1 id=&#34;univariate-optimized-rdd&#34;&gt;Univariate Optimized RDD&lt;/h1&gt;
&lt;p&gt;Now that we checked that our new sample is almost identical to the original sample, we can use our optimized RDD methodology on it. In this section I will use the univariate version of the optimized RDD method, estimating a model where the univariate running variable is the distance of the pixels to the national border. We previously computed the distance to the border, and I now modified that variable so the distance is negative for the pixels on the side of the border with lower Rule of Law or Control of Corruption.An advantage of doing this analysis first is that since we use the same running variable as the original paper, we should obtain comparable results, or at least more similar results than when we change to a multivariate running variable.&lt;/p&gt;
&lt;p&gt;To compute the curvature bound $B$ in this univariate case, we will follow Imbens and Wager (2019) and fit a global quadratic model for both treated and control samples. We get the absolute values of the coefficients, and then we keep as our curvature bound the maximum between the 2 quadratic coefficients. In this application there might be significant heterogeneity in the curvature between tribes, so I will also estimate the curvature within every tribe. I will discard the &lt;code&gt;NA&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; values obtained for the curvature bound, and then these values are going to be used for a sensitivity analysis. Specifically, for each treatment variable (high Rule of Law or high Control of Corruption) I will estimate the effect of institutions over local development using distance to the border as my running variable, and using the &lt;code&gt;optrdd&lt;/code&gt; package. I set the estimation point at 0 (the national border within every tribe). I will do this for 50 values of our curvature bound $B$, ranging from the minimum to the maximum of the within tribe curvatures computed with the global quadratic model. For each model, besides recovering the treatment effect, we also obtain a 95% confidence interval.&lt;/p&gt;
&lt;p&gt;In the 2 next figures we can see the results of these sensitivity analysis: we can notice that for both of our institutional quality treatment variables the confidence intervals contain 0, so neither of them is statistically significant for most of the values of $B$.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-17-1_hua1a67f4e40f0b9980a2ca6e7e12dcf26_4852_508ca7f912348364bf33f61877df3ecb.png 400w,
               /research/opt-rdd/unnamed-chunk-17-1_hua1a67f4e40f0b9980a2ca6e7e12dcf26_4852_4a1cc850446e20e0573b76cf39d618c5.png 760w,
               /research/opt-rdd/unnamed-chunk-17-1_hua1a67f4e40f0b9980a2ca6e7e12dcf26_4852_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-17-1_hua1a67f4e40f0b9980a2ca6e7e12dcf26_4852_508ca7f912348364bf33f61877df3ecb.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-17-2_hu7c041e0e9119edfa1266b6e0561a3891_5085_ec0fd185876b05f2796fc177c18468b6.png 400w,
               /research/opt-rdd/unnamed-chunk-17-2_hu7c041e0e9119edfa1266b6e0561a3891_5085_7ad5cacad13a3af818381f344d27c5c9.png 760w,
               /research/opt-rdd/unnamed-chunk-17-2_hu7c041e0e9119edfa1266b6e0561a3891_5085_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-17-2_hu7c041e0e9119edfa1266b6e0561a3891_5085_ec0fd185876b05f2796fc177c18468b6.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;We can also plot our weights to see how they look for a particular tribe (given the number of tribes, looking at all the tribes simultaneously is not feasible). To do this, I will estimate the model using the Rule of Law treatment variable, and for the curvature bound $B$ I use the curvature obtained when we use the global fit model for all the sample. In the following figure we can see the weights for the Azjer tribe, and we can notice that the weights for this tribe look a little weird. For the pixels in Libya there is a clear gradient, and the biggest weights are towards the border, whereas the weights in Algeria look weird, giving a lot of weight to pixels in the center of the partition, but not in the border.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-19-1_huddbadeec500e2a4da950f09e7c14a4f8_7224_eee9cb6d5ff96912a768284b6c8c71f2.png 400w,
               /research/opt-rdd/unnamed-chunk-19-1_huddbadeec500e2a4da950f09e7c14a4f8_7224_231afa69ffa5f6514661062e107a688a.png 760w,
               /research/opt-rdd/unnamed-chunk-19-1_huddbadeec500e2a4da950f09e7c14a4f8_7224_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-19-1_huddbadeec500e2a4da950f09e7c14a4f8_7224_eee9cb6d5ff96912a768284b6c8c71f2.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h1 id=&#34;multivariate-optimized-rdd&#34;&gt;Multivariate Optimized RDD&lt;/h1&gt;
&lt;p&gt;Now we will proceed to estimate a multivariate optimized RDD. We are going to estimate constant treatment effects, and we will use the latitude and longitude of the centroids of the pixels as the running variables.&lt;/p&gt;
&lt;p&gt;To estimate the curvature bound $B$, I&amp;rsquo;m going to follow the code from the geographic RDD example in Imbens and Wager (2019). Here I will use a slightly modified version of their  &lt;code&gt;get_curvature&lt;/code&gt; function, used to ran a cross-validated ridge regression with interacted 7th-order natural splines as features in each side of the border, and then use this to get a worst-case local curvature. Here I apply the function to the whole sample for both of the treatment variables, and I don&amp;rsquo;t estimate individual curvatures for the different tribes, since a lot of them have too little observations to produce reliable results with a technique like this.&lt;/p&gt;
&lt;p&gt;With this function, we can proceed to estimate our multivariate optimized RDD: for each treatment variable (high Rule of Law or high Control of Corruption) I will estimate the effect of institutions over local development using the latitude and longitude of my pixels as my running variables. I will do this for 20 values of our curvature bound, between $0.1B$ and $10B$, where the original curvature $B$ is computed with the &lt;code&gt;get_curvature&lt;/code&gt; function separately for each of the treatment variables. For each model, besides recovering the treatment effect, we also obtain a 95% confidence interval.&lt;/p&gt;
&lt;p&gt;In the two following figures we can see the results of these sensitivity analysis: we can notice that for Rule of Law generates a positive and mostly statistically significant effect, but when our treatment is defined by Control of Corruption the effect becomes not statistically significant.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-22-1_hu7d6ab9abb128d22c4815229ae2e93460_6329_89fa95b74691cde613f98bec073d20c5.png 400w,
               /research/opt-rdd/unnamed-chunk-22-1_hu7d6ab9abb128d22c4815229ae2e93460_6329_54327ea6e03b38419efe15811141d23f.png 760w,
               /research/opt-rdd/unnamed-chunk-22-1_hu7d6ab9abb128d22c4815229ae2e93460_6329_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-22-1_hu7d6ab9abb128d22c4815229ae2e93460_6329_89fa95b74691cde613f98bec073d20c5.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-22-2_hu7d7273af8461d51f94beea6d7dd56526_5374_1c04099e42e40c4f3de2cb9080f956f5.png 400w,
               /research/opt-rdd/unnamed-chunk-22-2_hu7d7273af8461d51f94beea6d7dd56526_5374_6d57fb611cda67cca08fe6b1eddadf45.png 760w,
               /research/opt-rdd/unnamed-chunk-22-2_hu7d7273af8461d51f94beea6d7dd56526_5374_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-22-2_hu7d7273af8461d51f94beea6d7dd56526_5374_1c04099e42e40c4f3de2cb9080f956f5.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Finally, we can also plot our weights to see how they look for a particular tribe. To do this, I will estimate the model using the Rule of Law treatment variable with maximum curvature $B$. In the next figure we can look again at the weights for the Azjer tribe, and in this case the pixels look a little weird, since there is not a clear gradient towards the border. I believe this is because this method might not work with multiple geographies at the same time, since there is nothing preventing the program to compare pixels among tribes and ignore the tribe borders.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /research/opt-rdd/unnamed-chunk-24-1_hud52c1b864a02edf90ded6a08270398b2_6795_2cb383e308ea9c00e0579bbe1df229c7.png 400w,
               /research/opt-rdd/unnamed-chunk-24-1_hud52c1b864a02edf90ded6a08270398b2_6795_c56ebd93763180b103c515d20dd1782a.png 760w,
               /research/opt-rdd/unnamed-chunk-24-1_hud52c1b864a02edf90ded6a08270398b2_6795_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/research/opt-rdd/unnamed-chunk-24-1_hud52c1b864a02edf90ded6a08270398b2_6795_2cb383e308ea9c00e0579bbe1df229c7.png&#34;
               width=&#34;672&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h1 id=&#34;concluding-remarks&#34;&gt;Concluding remarks&lt;/h1&gt;
&lt;p&gt;In this project I revisited the findings of Michalopoulos &amp;amp; Papaioannou (2014). I started by fixing some problems with their data, and then replicating their results with the new data to see if the replication was working properly. Using their methodology I found no statistically significant effect of institutions over local development in any of the samples.&lt;/p&gt;
&lt;p&gt;After that, I applied the Optimized Regression Discontinuity Design method with an univariate running variable, the distance to the border, using a wide range of curvature bounds $B$ derived from a within-tribe curvature analysis. Again I found no statistically significant effect.&lt;/p&gt;
&lt;p&gt;Finally, I applied the Optimized Regression Discontinuity Design method with an multivariate running variable, the latitude and longitude of pixels, and I derived the curvature bound using the non-parametric global method described in Imbens and Wager (2019). This time around I found that there is a positive and statistically significant effect of national institutions over development when we define institutions as Rule of Law, but that effect disappears when we measure institutions with Control of Corruption. These last findings are very relevant, since they challenge the previous evidence regarding this topic.&lt;/p&gt;
&lt;p&gt;However, I interpret these results with caution: I&amp;rsquo;m not fully sure of how well the multivariate running variable RDD method works here, since we are analyzing several geographical units at the same time, but the model doesn&amp;rsquo;t explicitly defines in which tribe a pixel is. This could lead to comparisons of pixels among different tribes, and the optimization process could end up ignoring completely the national borders that split tribes. There is more work to be done here to adapt this method to this geographical setting with multiple units, maybe related to re-centering the coordinates of each pixel, or adding more explicit constraints into the optimization problem.&lt;/p&gt;
&lt;p&gt;Another possible point to improve is the curvature bound $B$ for the multivariate case: I ended up estimating just 1 bound, so it would be very interesting to develop methods to estimate the curvature bound in a way that it takes into account the local curvatures for each tribe.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial: Landsat-8 image download and visualization using Google Earth Engine and Python</title>
      <link>https://nicolas-suarez.github.io/blog/landsat-8/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/blog/landsat-8/</guid>
      <description>&lt;p&gt;In this tutorial I want to explain how to download and visualize Landsat-8 images using GEE and Python packages. I wrote this code originally for my &lt;a href=&#34;https://github.com/nicolas-suarez/urban_emissions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Using satellite and street level images to predict urban emissions&amp;rdquo;&lt;/a&gt; project,  and I&amp;rsquo;m sharing it here because we struggled to download images in a simple and beginner friendly way. You can find the actual Jupyter notebook with this example and the proper outputs in &lt;a href=&#34;https://github.com/nicolas-suarez/landsat_8_tutorial/blob/main/landsat_8_tutorial_download.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;initial-set-up&#34;&gt;Initial set-up&lt;/h2&gt;
&lt;p&gt;We start by importing some packages. Besides installing the packages, we also need a Google-Earth Engine account to use the &lt;code&gt;ee&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import numpy as np
import pandas as pd
import pickle
import time
import math
import requests, zipfile, io
from geetools import cloud_mask
from IPython.display import Image

#importing Earth Engine packages
import ee #install in the console with &amp;quot;pip install earthengine-api --upgrade&amp;quot;
ee.Authenticate()  #every person needs an Earth Engine account to do this part
ee.Initialize()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;defining-satellite-images&#34;&gt;Defining satellite images&lt;/h2&gt;
&lt;p&gt;Here we are going to define our satellite images. In this example, we are going to use the Landsat-8 Surfarce reflectance Tier 1 image collection, containing all the images taken during 2020, and using the 3 RGB  bands for this example.&lt;/p&gt;
&lt;p&gt;Here we are also going to apply a mask to the pixels with the &lt;code&gt;cloud_mask&lt;/code&gt; function from the &lt;a href=&#34;https://pypi.org/project/geetools/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;geetools package&lt;/a&gt;. The mask allow us to exclude from our collection all the pixels that were covered by clouds, snow or similar sources of interference from an image. Since we are working for the moment with an image collection, we use the &lt;code&gt;map()&lt;/code&gt; function to individually apply the mask to all the elements in the collection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#defining image
startDate = &#39;2020-01-01&#39;
endDate = &#39;2020-12-31&#39;
landsat = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C01/T1_SR&amp;quot;)
# filter date
landsat = landsat.filterDate(startDate, endDate) 
#applying cloud masking
landsat_masked=landsat.map( cloud_mask.landsatSR([&#39;cloud&#39;]) )
#selecting bands
landsat_masked=landsat_masked.select([&amp;quot;B2&amp;quot;,&amp;quot;B3&amp;quot;,&amp;quot;B4&amp;quot;])
landsat = landsat.select([&amp;quot;B2&amp;quot;,&amp;quot;B3&amp;quot;,&amp;quot;B4&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;function-to-define-task-or-downloading-image-directly&#34;&gt;Function to define task or downloading image directly&lt;/h2&gt;
&lt;p&gt;After doing this, I&amp;rsquo;m going to define a function to download imagery called &lt;code&gt;image_task&lt;/code&gt;. This function download satellite images centered around a point defined by latitude and longitude coordinates, with a certain size, and allows us to store them either on a Google Drive folder, Google Cloud bucket or locally.&lt;/p&gt;
&lt;p&gt;We start by defining &lt;code&gt;len&lt;/code&gt;, the total size of the image in meters, that is computed as the product of the resolution of our Landsat-8 SR images (30 meters) and the number of pixels we want to capture in the image. After that, we generate a circle around the point of interested, using as the radius half of our &lt;code&gt;len&lt;/code&gt; parameter, and then we put a bounding box around the circle to get a square. We extract the coordinates of the bounding box and pass them to a &lt;code&gt;ee.Geometry.Rectangle&lt;/code&gt; object called &lt;code&gt;rectangle&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After that, we only need to crop our &lt;code&gt;rectangle&lt;/code&gt; object from our satellite image, called &lt;code&gt;image&lt;/code&gt; here. Here, our image is going to be our previously defined &lt;code&gt;ee.ImageCollection&lt;/code&gt; object containing our &lt;code&gt;Landstat-8 SR&lt;/code&gt; satellite images. We are going to clip our &lt;code&gt;rectangle&lt;/code&gt; area using the &lt;code&gt;filterBounds()&lt;/code&gt; method, and then we use the &lt;code&gt;mean()&lt;/code&gt; function to obtain a mean-composite of the images in the &lt;code&gt;ImageCollection&lt;/code&gt; for the specified area. We also need to define the &lt;code&gt;region&lt;/code&gt; where we are working (which is essentially the same as our &lt;code&gt;rectangle&lt;/code&gt;), and the dimensions of our image (the number of pixels for the width and height).&lt;/p&gt;
&lt;p&gt;Finally, the last step depends on how we want to download our images:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If we want to work locally with our files, then we are going to generate a download URL for our image in TIFF format, with the &lt;code&gt;ee.Image.getDownloadURL()&lt;/code&gt; method. We then use &lt;code&gt;requests&lt;/code&gt;, &lt;code&gt;zipfile&lt;/code&gt; and &lt;code&gt;io&lt;/code&gt; to download the image as a ZIP file, decompress it and store it a local folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we want to work with cloud services, like Google Drive or Google Cloud, which is more reliable when we work with large volumes of images, then we are going to use &lt;code&gt;ee.batch.Export.image.toCloudStorage()&lt;/code&gt; or &lt;code&gt;ee.batch.Export.image.toDrive()&lt;/code&gt; to create a Google Earth engine &lt;code&gt;task&lt;/code&gt; with specifications of what kind of image we want, and then we use &lt;code&gt;task.start()&lt;/code&gt; to start the task, and the images are going to appear in the specified cloud folders. We can also check the status of the task to see if the images were downloaded succesfully or not.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def image_download(image,point,image_res,n_pixels,folder_name, image_name, storage=&amp;quot;local&amp;quot;):
    
    &amp;quot;&amp;quot;&amp;quot;
    Function to download satellite images from a ee.imageCollection object.
    We first generate a bounding box of image_res*n_pixels meters around &amp;quot;point&amp;quot;,
    then we clip that region from the image collection, take the mean image from the collection,
    and send that as a task to the Google Earth Engine. 
    After that, we download the image Google Cloud Storage if storage==&amp;quot;Cloud&amp;quot;, 
    to Google Drive if storage==&amp;quot;Drive&amp;quot; or to a local folder if storage==&amp;quot;local&amp;quot;.
    
    Inputs:
    -image= ee.ImageCollection object
    -point= ee.Geometry.Point object
    -image_res= resolution of the image in meters
    -n_pixels= number of pixels to extract on the images
    -storage= string indicating if we are storing the images in Google Cloud,Google Drive or locally.
              Defaults to local storage.
    -folder_name= string with Google Cloud bucket name if storage==&amp;quot;Cloud&amp;quot;
                  string with the name of a folder in the root of Google Drive if storage==&amp;quot;Drive&amp;quot;
                  string with the path to the image if storage==&amp;quot;local&amp;quot;
    -image_name= string with the image_name for the TIFF image.

    Output:
     When storage==&amp;quot;Cloud&amp;quot; or storage==&amp;quot;Drive&amp;quot;:
     -task= an EE task object. we can then use task.status() to check the status of the task.
     If the task is completed, we will see a TIFF image in &amp;quot;folder_name&amp;quot; with name &amp;quot;image_name.tif&amp;quot;.
     The image has 3 dimensions, where the first 2 are n_pixels, and the 3rd is the number of bands of &amp;quot;image&amp;quot;.
     When storage==&amp;quot;local&amp;quot;:
     -there is no output, but we will see one TIFF file per band of our image in the folder &amp;quot;folder_name&amp;quot;.
    &amp;quot;&amp;quot;&amp;quot;
    #generating the box around the point
    len=image_res*n_pixels # for landsat, 30 meters * 224 pixels
    region= point.buffer(len/2).bounds().getInfo()[&#39;coordinates&#39;]
    #defining the rectangle
    coords=np.array(region)
    #taking min and maxs of coordinates to define the rectangle
    coords=[np.min(coords[:,:,0]), np.min(coords[:,:,1]), np.max(coords[:,:,0]), np.max(coords[:,:,1])]
    rectangle=ee.Geometry.Rectangle(coords)

    if storage==&amp;quot;Cloud&amp;quot;:
        #generating the export task (dimensions is &amp;quot;WIDTHxHEIGHT&amp;quot;)
        task=ee.batch.Export.image.toCloudStorage(image=image.filterBounds(rectangle).mean(), 
                            bucket=folder_name, 
                            description=image_name, 
                            region=str(region), dimensions=str(n_pixels)+&amp;quot;x&amp;quot;+str(n_pixels))
        #starting the task
        task.start()
        return task
    
    if storage==&amp;quot;Drive&amp;quot;:
        #generating the export task (dimensions is &amp;quot;WIDTHxHEIGHT&amp;quot;)
        task=ee.batch.Export.image.toDrive(image=image.filterBounds(rectangle).mean(), 
                            folder=folder_name, 
                            description=image_name, 
                            region=str(region), dimensions=str(n_pixels)+&amp;quot;x&amp;quot;+str(n_pixels))
        #starting the task
        task.start()
        return task
    
    if storage==&amp;quot;local&amp;quot;:
        #downloading the image
        r=requests.get( image.filterBounds(rectangle).mean().getDownloadURL({
                            &#39;name&#39;: image_name, 
                            &#39;region&#39;: str(region),
                            &#39;dimensions&#39;: str(n_pixels)+&amp;quot;x&amp;quot;+str(n_pixels)}))
        #unzip it to the selected directory
        z = zipfile.ZipFile(io.BytesIO(r.content))
        z.extractall(folder_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;testing-our-function&#34;&gt;Testing our function&lt;/h2&gt;
&lt;p&gt;To test how our function works, we are going to create a little example, taking a 6.72 by 6.72 km satellite image around the Oval in Stanford campus, and storing it locally:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#defining the oval as a point
oval=ee.Geometry.Point(-122.169678,37.429154)

#running our function
image_download(image=landsat,point=oval,image_res=30,n_pixels=224,folder_name=&#39;LS8_images&#39;, image_name=&#39;test_image&#39;, storage=&amp;quot;local&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are going to test that the images were indeed downloaded to the &lt;code&gt;LS8_images&lt;/code&gt; folder:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;os.listdir(&#39;LS8_images&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed see that there are 3 files in our folder, one for each channel of the image.&lt;/p&gt;
&lt;p&gt;We could further work with this images. For instance, we can use packages like imageio to read the images in Python, read them into a numpy array and then use numpy functions like stack to stack all individual images into one big array.&lt;/p&gt;
&lt;h1 id=&#34;visualizing-images&#34;&gt;Visualizing images&lt;/h1&gt;
&lt;p&gt;To visualize images, we are going to do very similar things to what we did before. I will generate the satellite images in almost the same way I did before, but now we are going to add some visualization parameters, &lt;code&gt;visParams&lt;/code&gt; to properly see our images.&lt;/p&gt;
&lt;p&gt;We are going to obtain a &lt;code&gt;jpg&lt;/code&gt; image using the &lt;code&gt;ee.Image.getThumbUrl()&lt;/code&gt;function, and we are going to visualize directly here. The function programmed below allow us to decide if we want to see a cloud masked version of the image or the regular one.&lt;/p&gt;
&lt;p&gt;The visualization parameters and the image size are defined here, but could be modified if we wanted.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#defining parameters for the function
image_res=30
n_pixels=224

#visualization parameters
visParams={&#39;min&#39;: 0, &#39;max&#39;: 3000, &#39;gamma&#39;: 1.4,  
           &#39;bands&#39; : [&#39;B4&#39;, &#39;B3&#39;, &#39;B2&#39;], &#39;dimensions&#39; : str(n_pixels)+&amp;quot;x&amp;quot;+str(n_pixels),
           &#39;format&#39; : &#39;jpg&#39;}

#defining the function
def visualization(point,name,mask=True):
    &#39;&#39;&#39;
    Function to visualize the images for our ML application.
    Inputs:
        -point= ee.Geometry.point object
        -name: name that is going to be given to the jpg file
        -mask: True to get masked image, False to get unmasked image
    Outputs:
        The function doesn&#39;t produce an output, but generates a file called
        &amp;quot;name.jpg&amp;quot; in the current directory
    &#39;&#39;&#39;
    #computing bounding box
    len=image_res*n_pixels # for landsat, 30 meters * 224 pixels
    region= point.buffer(len/2).bounds().getInfo()[&#39;coordinates&#39;]
    coords=np.array(region)
    coords=[np.min(coords[:,:,0]), np.min(coords[:,:,1]), np.max(coords[:,:,0]), np.max(coords[:,:,1])]
    rectangle=ee.Geometry.Rectangle(coords)
    
    #clipping the area from satellite image
    if mask==True:
        clipped_image= landsat_masked.mean().clip(rectangle)
    else:
        clipped_image= landsat.mean().clip(rectangle)
        
    #getting the image
    requests.get(clipped_image.getThumbUrl(visParams))
    open(name+&#39;.jpg&#39;, &#39;wb&#39;).write(requests.get(clipped_image.getThumbUrl(visParams)).content)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are going to test the code. First we will visualize our image around the Oval without cloud masking:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;visualization(point=oval,name=&#39;oval_no_mask&#39;,mask=False)
Image(filename=&#39;oval_no_mask.jpg&#39;) 
&lt;/code&gt;&lt;/pre&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/landsat-8/oval_no_mask_hu925074f9182ffee7a618036463d8005d_25765_5888580dcdc71f8b2894e2d5b1a59c54.jpg 400w,
               /blog/landsat-8/oval_no_mask_hu925074f9182ffee7a618036463d8005d_25765_d06e44af9b5eaa2d32149c1a2ffddf0d.jpg 760w,
               /blog/landsat-8/oval_no_mask_hu925074f9182ffee7a618036463d8005d_25765_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/blog/landsat-8/oval_no_mask_hu925074f9182ffee7a618036463d8005d_25765_5888580dcdc71f8b2894e2d5b1a59c54.jpg&#34;
               width=&#34;224&#34;
               height=&#34;224&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;And now we are going to visualize our image applying cloud masking:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;visualization(point=oval,name=&#39;oval_cloud_masking&#39;,mask=True)
Image(filename=&#39;oval_cloud_masking.jpg&#39;) 
&lt;/code&gt;&lt;/pre&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/landsat-8/oval_cloud_masking_huef47ff7ed2b8cadf0a63b1ff93a97438_31059_3b79d85081fe32d71f43bd7be89ebbc0.jpg 400w,
               /blog/landsat-8/oval_cloud_masking_huef47ff7ed2b8cadf0a63b1ff93a97438_31059_8a60a6f13e6d14e1db8b0de00a75b557.jpg 760w,
               /blog/landsat-8/oval_cloud_masking_huef47ff7ed2b8cadf0a63b1ff93a97438_31059_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/blog/landsat-8/oval_cloud_masking_huef47ff7ed2b8cadf0a63b1ff93a97438_31059_3b79d85081fe32d71f43bd7be89ebbc0.jpg&#34;
               width=&#34;224&#34;
               height=&#34;224&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Ground Level Ozone Concentration from Urban Satellite and Street Level Imagery using Multimodal CNN</title>
      <link>https://nicolas-suarez.github.io/research/urban-emissions/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/urban-emissions/</guid>
      <description>&lt;p&gt;This was our class project for Stanford CS230 &amp;ldquo;Deep Learning&amp;rdquo; class during the Winter 2021 quarter. The project was featured as one of the &lt;a href=&#34;https://cs230.stanford.edu/past-projects/#winter-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Outstanding projects for the Winter 2021 quarter&lt;/a&gt;. You can find our final report &lt;a href=&#34;http://cs230.stanford.edu/projects_winter_2021/reports/70701113.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;This project examines the relationship between the level of ozone concentration
in urban locations and their physical features through the use of Convolutional
Neural Networks (CNNs). We train two models, including one trained on satellite
imagery (&amp;ldquo;Satellite CNN&amp;rdquo;) to capture higher-level features such as the location&amp;rsquo;s 
geography, and the other trained on street-level imagery (&amp;ldquo;Street CNN&amp;rdquo;) to learn
ground-level features such as motor vehicle activity. These features are then 
concatenated to train neural network (&amp;ldquo;Concat NN&amp;rdquo;) on this shared representation
and predict the location&amp;rsquo;s level of ozone as measured in parts per billion.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;We obtained ozone measurements (parts per billion) for 12,976 semi-unique locations with ozone levels information mostly located in North America from &lt;a href=&#34;https://www.airnow.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AirNow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our satellite imagery dataset was constructed using the Google Earth Engine API: for each location labeled with an ozone reading, we retrieve one satellite image centered at that location from the Landsat 8 Surface Reflectance Tier 1 Collection with a resolution of 224 $\times$ 224 pixels which represents 6.72 km $\times$ 6.72 km.  We use 7 bands from this collection: RGB, ultra blue, near infrared, and two shortwave infrared bands. We preprocess each of our images by adding a cloud mask per pixel and then computing the per pixel and band mean composite of all the available images for the year 2020.&lt;/p&gt;
&lt;p&gt;The street-level imagery dataset was constructed using the Google Maps Street View API. For each location labeled with an ozone level, we randomly sample 10 geospatial points within 6.72 km from the measurement point.&lt;/p&gt;
&lt;p&gt;Here we can see some examples from our dataset:&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_4b1e4aca95726f2a02631b2ae83c38f1.jpg 400w,
               /media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_5f62ffe314db713f03717d4b1fcb3af1.jpg 760w,
               /media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_4b1e4aca95726f2a02631b2ae83c38f1.jpg&#34;
               width=&#34;760&#34;
               height=&#34;240&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;network-architecture&#34;&gt;Network architecture&lt;/h2&gt;
&lt;p&gt;We train the two CNNs separately on the satellite and street-level imagery, both using a ResNet-18 architecture implemented in PyTorch and pretrained on the ImageNet dataset. The models are trained separately as the nature of the features they need to learn to associate with ozone concentration is quite different for each dataset. Transfer learning is used for both CNNs to leverage lower-level features learned on the ImageNet dataset. The ResNet-18 architecture was slightly adapted for our particular task; in the case of the satellite imagery, the CNN&amp;rsquo;s input layer was modified to accommodate for the image&amp;rsquo;s seven channels and was initialized using Kaiming initialization.&lt;/p&gt;
&lt;p&gt;After training both CNNs separately to predict the ozone reading for each location, we extract 512 features for each satellite and each street image. These are concatenated to create a feature vector of size 1,024 representing the satellite image and a particular street view of a given location. We then train a Concatenated Feedforward Neural Network (NN) using these multiple representations of each location to predict the location&amp;rsquo;s average ozone level in 2020.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_dd4259d59b8b8566889798be3f420a90.PNG 400w,
               /media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_4d173af12e1b73c91b367554e273a95f.PNG 760w,
               /media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_dd4259d59b8b8566889798be3f420a90.PNG&#34;
               width=&#34;760&#34;
               height=&#34;330&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;More details about regularization, the tuning process of hyperparameters and training of the network can be found in the report.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;After tuning our hyperparameters and training our models, we obtain the following performance (Root Mean Square Error in our test set):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Satellite Model&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Street-level Model&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Concatenated Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Test RMSE (ppb)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12.48&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20.64&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11.70&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can also visually compare our predictions for the test with ground truth values in the following figure:&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_50b966737bd7f132fe17c211944a05a2.jpg 400w,
               /media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_988aa65ea5f8b0a6a2a942f0f83b494e.jpg 760w,
               /media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_50b966737bd7f132fe17c211944a05a2.jpg&#34;
               width=&#34;760&#34;
               height=&#34;312&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>A little Stata command to export tables to Excel</title>
      <link>https://nicolas-suarez.github.io/blog/stata-tables/</link>
      <pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/blog/stata-tables/</guid>
      <description>&lt;p&gt;I usually prefer to work in LaTeX, but from time to time I&amp;rsquo;m forced to share my results in Excel. In Stata, I used to use the &lt;a href=&#34;http://repec.org/bocode/o/outreg2.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Outreg2 command&lt;/a&gt;, but the output generated by this program has a weird format, and you always have to read some error messages in Excel.&lt;/p&gt;
&lt;p&gt;Because of this, I decided to program a little piece of code to easily export my regression results to Excel. I&amp;rsquo;m not very creative, so I called my custom command &lt;strong&gt;export_tables&lt;/strong&gt;. You can find the code at my &lt;a href=&#34;https://github.com/nicolas-suarez/Stata-table_export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github page&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;syntax&#34;&gt;Syntax&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;export_tables, Models(string) Dec(real) Cell(string) USING(string) SHEET(string) [ Options ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Models() contains a list of the models you estimated, and saved with the &amp;ldquo;Estimates Store&amp;rdquo; command.&lt;/li&gt;
&lt;li&gt;Dec() indicates the number of decimal places of your model&amp;rsquo;s coefficients.&lt;/li&gt;
&lt;li&gt;Cell() indicates in which Excel&amp;rsquo;s cell the table is going to begin.&lt;/li&gt;
&lt;li&gt;Using() indicates where are you going to save the excel document, and Sheet() indicates how the sheet of the document will be named.&lt;/li&gt;
&lt;li&gt;In options, you can use Keep() to keep the coefficients associated with certain variables, Drop() to drop the coefficients associated with some variables, Stats() to report some e() statistics stored in your models and Dstats() helps you to set the number of decimal places associated with these statistics. Variables prints the name of the variables in your table, and label prints the variables&#39; labels instead. Std to display standard errors below the coefficients, Pvalues to display the p-values of the coefficients below them, and See to print your output in the console as a table.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-use-the-command&#34;&gt;How to use the command&lt;/h2&gt;
&lt;p&gt;The idea behind the command is to run your regressions, store the results with estimates store, and then export the results to excel. You will get a table without a header, but you will see in every row the name of the variable, the coefficients of each model (with stars representing the significance level), the standard errors, and at the end, some statitics. After running this command, you can use &lt;a href=&#34;https://www.stata.com/manuals13/pputexcel.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;putexcel&lt;/a&gt; to further modify your spreadsheet, adding headers, bold or italic text, borders, etc.&lt;/p&gt;
&lt;p&gt;For the moment, this command only works with regression commands like &lt;strong&gt;reg&lt;/strong&gt;, &lt;strong&gt;probit&lt;/strong&gt;, &lt;strong&gt;ivreg2&lt;/strong&gt; or others, but doesn&amp;rsquo;t work with commands like &lt;strong&gt;heckman&lt;/strong&gt; or others that display the results of 2 or more estimations at the same time.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;You can run the following example code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;sysuse auto

reg mpg rep78 trunk
est sto m1

reg turn length gear_ratio rep78
est sto m2

export_tables, models(m1 m2) dec(2) cell(A1) using(&amp;quot;test.xlsx&amp;quot;) sheet(&amp;quot;sheet1&amp;quot;) stats(&amp;quot;N r2_a&amp;quot;) label std see
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could also use a wilcard in the models, and write something like &lt;code&gt;models(m*)&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inequality in social capital in Chile: Assessing the importance of network size and contacts’ occupational prestige on status attainment</title>
      <link>https://nicolas-suarez.github.io/research/social-networks/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/social-networks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The impact of commuting time over educational achievement: A machine learning approach</title>
      <link>https://nicolas-suarez.github.io/research/commuting-time/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/commuting-time/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://nicolas-suarez.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
