<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3 | Nicolás Suárez Chavarría</title>
    <link>https://nicolas-suarez.github.io/publication-type/3/</link>
      <atom:link href="https://nicolas-suarez.github.io/publication-type/3/index.xml" rel="self" type="application/rss+xml" />
    <description>3</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 18 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://nicolas-suarez.github.io/media/icon_hu5c88e8a9398cf657f3e83a0d9ccab3ac_75464_512x512_fill_lanczos_center_3.png</url>
      <title>3</title>
      <link>https://nicolas-suarez.github.io/publication-type/3/</link>
    </image>
    
    <item>
      <title>Predicting Ground Level Ozone Concentration from Urban Satellite and Street Level Imagery using Multimodal CNN</title>
      <link>https://nicolas-suarez.github.io/research/urban-emissions/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/urban-emissions/</guid>
      <description>&lt;p&gt;This was our class project for Stanford CS230 &amp;ldquo;Deep Learning&amp;rdquo; class during the Winter 2021 quarter. The project was featured as one of the &lt;a href=&#34;https://cs230.stanford.edu/past-projects/#winter-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Outstanding projects for the Winter 2021 quarter&lt;/a&gt;. You can find our final report &lt;a href=&#34;http://cs230.stanford.edu/projects_winter_2021/reports/70701113.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;This project examines the relationship between the level of ozone concentration
in urban locations and their physical features through the use of Convolutional
Neural Networks (CNNs). We train two models, including one trained on satellite
imagery (&amp;ldquo;Satellite CNN&amp;rdquo;) to capture higher-level features such as the location&amp;rsquo;s 
geography, and the other trained on street-level imagery (&amp;ldquo;Street CNN&amp;rdquo;) to learn
ground-level features such as motor vehicle activity. These features are then 
concatenated to train neural network (&amp;ldquo;Concat NN&amp;rdquo;) on this shared representation
and predict the location&amp;rsquo;s level of ozone as measured in parts per billion.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;We obtained ozone measurements (parts per billion) for 12,976 semi-unique locations with ozone levels information mostly located in North America from &lt;a href=&#34;https://www.airnow.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AirNow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our satellite imagery dataset was constructed using the Google Earth Engine API: for each location labeled with an ozone reading, we retrieve one satellite image centered at that location from the Landsat 8 Surface Reflectance Tier 1 Collection with a resolution of 224 $\times$ by 224 pixels which represents 6.72 km  by 6.72 km.  We use 7 bands from this collection: RGB, ultra blue, near infrared, and two shortwave infrared bands. We preprocess each of our images by adding a cloud mask per pixel and then computing the per pixel and band mean composite of all the available images for the year 2020.&lt;/p&gt;
&lt;p&gt;The street-level imagery dataset was constructed using the Google Maps Street View API. For each location labeled with an ozone level, we randomly sample 10 geospatial points within 6.72 km from the measurement point.&lt;/p&gt;
&lt;p&gt;Here we can see some examples from our dataset:&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_4b1e4aca95726f2a02631b2ae83c38f1.jpg 400w,
               /media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_5f62ffe314db713f03717d4b1fcb3af1.jpg 760w,
               /media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/table1_huf86a670c5ac72a7964fb834b7736087e_57419_4b1e4aca95726f2a02631b2ae83c38f1.jpg&#34;
               width=&#34;760&#34;
               height=&#34;240&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;network-architecture&#34;&gt;Network architecture&lt;/h2&gt;
&lt;p&gt;We train the two CNNs separately on the satellite and street-level imagery, both using a ResNet-18 architecture implemented in PyTorch and pretrained on the ImageNet dataset. The models are trained separately as the nature of the features they need to learn to associate with ozone concentration is quite different for each dataset. Transfer learning is used for both CNNs to leverage lower-level features learned on the ImageNet dataset. The ResNet-18 architecture was slightly adapted for our particular task; in the case of the satellite imagery, the CNN&amp;rsquo;s input layer was modified to accommodate for the image&amp;rsquo;s seven channels and was initialized using Kaiming initialization.&lt;/p&gt;
&lt;p&gt;After training both CNNs separately to predict the ozone reading for each location, we extract 512 features for each satellite and each street image. These are concatenated to create a feature vector of size 1,024 representing the satellite image and a particular street view of a given location. We then train a Concatenated Feedforward Neural Network (NN) using these multiple representations of each location to predict the location&amp;rsquo;s average ozone level in 2020.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_dd4259d59b8b8566889798be3f420a90.PNG 400w,
               /media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_4d173af12e1b73c91b367554e273a95f.PNG 760w,
               /media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/architecture4_hu33a4bc16cd2fddcf91d99bb7d2037609_163418_dd4259d59b8b8566889798be3f420a90.PNG&#34;
               width=&#34;760&#34;
               height=&#34;330&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;More details about regularization, the tuning process of hyperparameters and training of the network can be found in the report.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;After tuning our hyperparameters and training our models, we obtain the following performance (Root Mean Square Error in our test set):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Satellite Model&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Street-level Model&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Concatenated Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Test RMSE (ppb)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12.48&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20.64&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11.70&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can also visually compare our predictions for the test with ground truth values in the following figure:&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_50b966737bd7f132fe17c211944a05a2.jpg 400w,
               /media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_988aa65ea5f8b0a6a2a942f0f83b494e.jpg 760w,
               /media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://nicolas-suarez.github.io/media/table4_huc85fa806f0b72bdf75cb9f06cc6195a7_51089_50b966737bd7f132fe17c211944a05a2.jpg&#34;
               width=&#34;760&#34;
               height=&#34;312&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>The impact of commuting time over educational achievement: A machine learning approach</title>
      <link>https://nicolas-suarez.github.io/research/commuting-time/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://nicolas-suarez.github.io/research/commuting-time/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
